% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/simulation_diagnostics.R
\name{run_benchmark}
\alias{run_benchmark}
\title{Run Monte Carlo benchmark for Safe-U and conformal Safe-U}
\usage{
run_benchmark(
  B = 200,
  m = 2000,
  n = 5,
  pi0 = 0.2,
  target_power = 0.35,
  alpha = 0.1
)
}
\arguments{
\item{B}{Integer, number of Monte Carlo replicates.}

\item{m}{Number of hypotheses per replicate.}

\item{n}{Sample size per group for each t-test.}

\item{pi0}{Proportion of nulls in the simulator.}

\item{target_power}{Target power used to calibrate non-null effects.}

\item{alpha}{Target FDR level.}
}
\value{
A data frame with one row per replicate and columns:
\code{Umin_safe}, \code{Umin_conf}, \code{disc_safe},
\code{disc_conf}, \code{fdp_safe}, \code{fdp_conf}. The function
also prints summary statistics to the console.
}
\description{
Runs a Monte Carlo simulation comparing the baseline Safe-U procedure
to its conformal refinement using a pre-trained CQR model for \code{pi0}.
Internally, this function:
}
\details{
\enumerate{
\item Generates calibration data using \code{\link[=generate_calibration_data]{generate_calibration_data()}},
\item Trains a CQR model via \code{\link[=train_cqr_pi0]{train_cqr_pi0()}},
\item Repeats \code{B} independent simulation runs and for each,
computes summary statistics (minimum per-fold \code{U},
number of discoveries, and FDP for each method).
}
}
\examples{
\dontrun{
# Small benchmark (B = 20) just to check everything runs
tab <- run_benchmark(B = 20, m = 1000, n = 5, pi0 = 0.2,
                     target_power = 0.35, alpha = 0.10)
head(tab)
}

}
